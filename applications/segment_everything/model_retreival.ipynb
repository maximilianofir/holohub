{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install opencv-python matplotlib onnxruntime onnx\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import SamPredictor, sam_model_registry, SamAutomaticMaskGenerator\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from segment_anything.utils.onnx import SamOnnxModel\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization import QuantType\n",
    "from onnxruntime.quantization.quantize import quantize_dynamic\n",
    "\n",
    "import torch\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a model\n",
    "# !python -m wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
    "sam = sam_model_registry[\"vit_b\"](R\"C:\\Users\\mmoller\\Downloads\\sam_vit_b_01ec64.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the image of the path into an ndarray using PIL and set it as the image to be processed\n",
    "image_path = R\"C:\\Users\\mmoller\\OneDrive - NVIDIA Corporation\\Pictures\\Camera Roll\\WIN_20240318_17_50_49_Pro.jpg\"\n",
    "image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTensor:\n",
    "    def __init__(self, image):\n",
    "        self.image = image\n",
    "        self.orig_width, self.orig_height = image.size\n",
    "        self.resized_width, self.resized_height = None, None\n",
    "        self.pad_width, self.pad_height = None, None\n",
    "\n",
    "    def size(self):\n",
    "        return self.image.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_object = ImageTensor(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePreprocessor:\n",
    "    def __init__(self, long_side_max=1024, mean=None, std=None, image_format=\"RGB\", pad_to_square=True):\n",
    "        self.long_side_max = long_side_max\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.image_format = image_format\n",
    "        self.pad_to_square = pad_to_square\n",
    "        if self.mean is None:\n",
    "            self.mean = np.array([123.675, 116.28, 103.53])\n",
    "        if self.std is None:\n",
    "            self.std = np.array([58.395, 57.12, 57.375])\n",
    "\n",
    "\n",
    "    def resize_image_to_long_side(self, img: ImageTensor):\n",
    "        if self.long_side_max is None:\n",
    "            return img\n",
    "        orig_width, orig_height = img.image.size\n",
    "        if orig_width > orig_height:\n",
    "            img.resized_width = self.long_side_max\n",
    "            img.resized_height = int(self.long_side_max / orig_width * orig_height)\n",
    "        else:\n",
    "            img.resized_height = self.long_side_max\n",
    "            img.resized_width = int(self.long_side_max / orig_height * orig_width)\n",
    "\n",
    "        img.image = img.image.resize((img.resized_width, img.resized_height), Image.Resampling.BILINEAR)\n",
    "        return img\n",
    "\n",
    "    def make_image_rgb(self, image):\n",
    "        if image.image.mode == \"RGB\":\n",
    "            return image\n",
    "        return image.image.convert(\"RGB\")\n",
    "\n",
    "    def pad_image_to_square(self, image):\n",
    "        if isinstance(image, ImageTensor):\n",
    "            image.image = self.pad_image_to_square(image.image)\n",
    "            return image\n",
    "        else:\n",
    "            h, w = image.shape[2:]\n",
    "            max_dim = max(h, w)\n",
    "            pad_h = max_dim - h\n",
    "            pad_w = max_dim - w\n",
    "            image = np.pad(image, ((0,0), (0,0), (0,pad_h), (0,pad_w)), mode=\"constant\", constant_values=0)\n",
    "            return image\n",
    "\n",
    "    def normalize_image(self, image):\n",
    "        if isinstance(image, ImageTensor):\n",
    "            image.image = self.normalize_image(image.image)\n",
    "            return image\n",
    "        else:\n",
    "            image = (image - self.mean) / self.std\n",
    "            return image\n",
    "\n",
    "    def to_tensor(self, image):\n",
    "        if isinstance(image, ImageTensor):\n",
    "            image.image = self.to_tensor(image.image)\n",
    "            return image\n",
    "        else:\n",
    "            image = image.transpose(2,0,1)[None,:,:,:].astype(np.float32)\n",
    "            return image\n",
    "\n",
    "    def from_image_to_input(self, image):\n",
    "        image = self.make_image_rgb(image)\n",
    "        image = self.resize_image_to_long_side(image)\n",
    "        image = self.normalize_image(image)\n",
    "        image = self.to_tensor(image)\n",
    "        # pad to square\n",
    "        if self.pad_to_square:\n",
    "            image = self.pad_image_to_square(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_preprocessor = ImagePreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = image_preprocessor.from_image_to_input(image_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image.resized_width, input_image.resized_height, input_image.pad_width, input_image.pad_height, input_image.orig_width, input_image.orig_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(input_image.image[0].transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SamPredictor(sam)\n",
    "predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "masks = mask_generator.generate(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(image)\n",
    "show_anns(masks)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export an ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "checkpoint_path =os.path.join(R\"C:\\\\Users\\\\mmoller\\\\Downloads\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = sam_model_registry[model_type](checkpoint=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"sam_onnx_example.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "onnx_model_path = \"sam_onnx_example.onnx\"\n",
    "\n",
    "onnx_model = SamOnnxModel(sam, return_single_mask=True)\n",
    "\n",
    "dynamic_axes = {\n",
    "    \"point_coords\": {1: \"num_points\"},\n",
    "    \"point_labels\": {1: \"num_points\"},\n",
    "}\n",
    "\n",
    "embed_dim = sam.prompt_encoder.embed_dim\n",
    "embed_size = sam.prompt_encoder.image_embedding_size\n",
    "mask_input_size = [4 * x for x in embed_size]\n",
    "dummy_inputs = {\n",
    "    \"image_embeddings\": torch.randn(1, embed_dim, *embed_size, dtype=torch.float),\n",
    "    \"point_coords\": torch.randint(low=0, high=1024, size=(1, 5, 2), dtype=torch.float),\n",
    "    \"point_labels\": torch.randint(low=0, high=4, size=(1, 5), dtype=torch.float),\n",
    "    \"mask_input\": torch.randn(1, 1, *mask_input_size, dtype=torch.float),\n",
    "    \"has_mask_input\": torch.tensor([1], dtype=torch.float),\n",
    "    \"orig_im_size\": torch.tensor([1500, 2250], dtype=torch.float),\n",
    "}\n",
    "output_names = [\"masks\", \"iou_predictions\", \"low_res_masks\"]\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    with open(onnx_model_path, \"wb\") as f:\n",
    "        torch.onnx.export(\n",
    "            onnx_model,\n",
    "            tuple(dummy_inputs.values()),\n",
    "            f,\n",
    "            export_params=True,\n",
    "            verbose=False,\n",
    "            opset_version=17,\n",
    "            do_constant_folding=True,\n",
    "            input_names=list(dummy_inputs.keys()),\n",
    "            output_names=output_names,\n",
    "            dynamic_axes=dynamic_axes,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using the local onnx model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "checkpoint_path =os.path.join(R\"C:\\\\Users\\\\mmoller\\\\Downloads\", checkpoint)\n",
    "sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
    "ort_session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "sam.to(device='cuda')\n",
    "predictor = SamPredictor(sam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "masks = mask_generator.generate(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(image)\n",
    "show_anns(masks)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export the image encoder to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAM model\n",
    "sam = sam_model_registry[\"vit_b\"](R\"C:\\Users\\mmoller\\Downloads\\sam_vit_b_01ec64.pth\")\n",
    "\n",
    "# Export images encoder from SAM model to ONNX\n",
    "torch.onnx.export(\n",
    "    f=\"vit_b_encoder.onnx\",\n",
    "    model=sam.image_encoder,\n",
    "    args=torch.randn(1, 3, 1024, 1024),\n",
    "    input_names=[\"images\"],\n",
    "    output_names=[\"embeddings\"],\n",
    "    export_params=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing encoder and decoder on an image\n",
    "\n",
    "encoder = onnxruntime.InferenceSession(\"vit_b_encoder.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = encoder.run(None, {\"images\": input_image.image})\n",
    "embeddings = outputs[0]\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instructions from https://github.com/facebookresearch/segment-anything/blob/main/notebooks/onnx_model_example.ipynb\n",
    "The ONNX model has a different input signature than SamPredictor.predict. The following inputs must all be supplied. Note the special cases for both point and mask inputs. All inputs are np.float32.\n",
    "\n",
    "- image_embeddings: The image embedding from predictor.get_image_embedding(). Has a batch index of length 1.\n",
    "- point_coords: Coordinates of sparse input prompts, corresponding to both point inputs and box inputs. Boxes are encoded using two points, one for the top-left corner and one for the bottom-right corner. Coordinates must already be transformed to long-side 1024. Has a batch index of length 1.\n",
    "- point_labels: Labels for the sparse input prompts. 0 is a negative input point, 1 is a positive input point, 2 is a top-left box corner, 3 is a bottom-right box corner, and -1 is a padding point. If there is no box input, a single padding point with label -1 and coordinates (0.0, 0.0) should be concatenated.\n",
    "- mask_input: A mask input to the model with shape 1x1x256x256. This must be supplied even if there is no mask input. In this case, it can just be zeros.\n",
    "- has_mask_input: An indicator for the mask input. 1 indicates a mask input, 0 indicates no mask input.\n",
    "- orig_im_size: The size of the input image in (H,W) format, before any transformation.\n",
    "\n",
    "Additionally, the ONNX model does not threshold the output mask logits. To obtain a binary mask, threshold at sam.mask_threshold (equal to 0.0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the entire image as the query\n",
    "input_point = np.array([[0,0],  [1024, 1024]])\n",
    "input_label = np.array([1, 1])\n",
    "onnx_coord = np.concatenate([input_point, np.array([[0.0, 0.0]])], axis=0)[None, :, :]\n",
    "onnx_label = np.concatenate([input_label, np.array([-1])], axis=0)[None, :].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image.re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_box = np.array([0, 0, 1000, 1025]).reshape(2,2)\n",
    "box_labels = np.array([2,3])\n",
    "input_point = np.array([[140, 160]])\n",
    "input_label = np.array([0])\n",
    "\n",
    "onnx_coord = np.concatenate([input_point, input_box], axis=0)[None, :, :]\n",
    "onnx_label = np.concatenate([input_label, box_labels], axis=0)[None, :].astype(np.float32)\n",
    "\n",
    "coords = deepcopy(onnx_coord).astype(float)\n",
    "coords[..., 0] = coords[..., 0] * (input_image.resized_height / input_image.orig_height)\n",
    "coords[..., 1] = coords[..., 1] * (input_image.resized_width / input_image.orig_width)\n",
    "\n",
    "onnx_coord = coords.astype(\"float32\")\n",
    "onnx_coord\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = onnxruntime.InferenceSession(\"sam_onnx_example.onnx\")\n",
    "onnx_mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n",
    "onnx_has_mask_input = np.zeros(1, dtype=np.float32)\n",
    "\n",
    "outputs = decoder.run(None,{\n",
    "    \"image_embeddings\": embeddings,\n",
    "    \"point_coords\": onnx_coord,\n",
    "    \"point_labels\": onnx_label,\n",
    "    \"mask_input\": onnx_mask_input,\n",
    "    \"has_mask_input\": onnx_has_mask_input,\n",
    "    \"orig_im_size\": np.array([input_image.resized_height, input_image.resized_width], dtype=np.float32),\n",
    "})\n",
    "masks = outputs[0]\n",
    "masks.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = masks[0][0]\n",
    "mask = (mask > 0).astype('uint8')*255\n",
    "img = Image.fromarray(mask,'L')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
