{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install opencv-python matplotlib onnxruntime onnx\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import SamPredictor, sam_model_registry, SamAutomaticMaskGenerator\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from segment_anything.utils.onnx import SamOnnxModel\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization import QuantType\n",
    "from onnxruntime.quantization.quantize import quantize_dynamic\n",
    "\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "\n",
    "from typing import Tuple\n",
    "import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n",
    "\n",
    "\n",
    "def show_mask(mask, ax):\n",
    "    color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n",
    "\n",
    "\n",
    "class ImageTensor:\n",
    "    def __init__(self, image):\n",
    "        self.image = image\n",
    "        self.orig_width, self.orig_height = image.size\n",
    "        self.resized_width, self.resized_height = None, None\n",
    "        self.pad_width, self.pad_height = None, None\n",
    "\n",
    "    def size(self):\n",
    "        return self.image.size\n",
    "\n",
    "    def apply_coords(self, coords: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Expects a numpy array of length 2 in the final dimension\n",
    "        \"\"\"\n",
    "        old_h, old_w = self.orig_height, self.orig_width\n",
    "        new_h, new_w = self.resized_height, self.resized_width\n",
    "        coords = deepcopy(coords).astype(float)\n",
    "        coords[..., 0] = coords[..., 0] * (new_w / old_w)\n",
    "        coords[..., 1] = coords[..., 1] * (new_h / old_h)\n",
    "        return coords\n",
    "\n",
    "\n",
    "class ImagePreprocessor:\n",
    "    def __init__(self, long_side_max=1024, mean=None, std=None, image_format=\"RGB\", pad_to_square=True):\n",
    "        self.long_side_max = long_side_max\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.image_format = image_format\n",
    "        self.pad_to_square = pad_to_square\n",
    "        if self.mean is None:\n",
    "            self.mean = np.array([123.675, 116.28, 103.53])\n",
    "        if self.std is None:\n",
    "            self.std = np.array([58.395, 57.12, 57.375])\n",
    "\n",
    "\n",
    "    def resize_image_to_long_side(self, img: ImageTensor):\n",
    "        if self.long_side_max is None:\n",
    "            return img\n",
    "        orig_width, orig_height = img.image.size\n",
    "        if orig_width > orig_height:\n",
    "            img.resized_width = self.long_side_max\n",
    "            img.resized_height = int(self.long_side_max / orig_width * orig_height)\n",
    "        else:\n",
    "            img.resized_height = self.long_side_max\n",
    "            img.resized_width = int(self.long_side_max / orig_height * orig_width)\n",
    "\n",
    "        img.image = img.image.resize((img.resized_width, img.resized_height), Image.Resampling.BILINEAR)\n",
    "        return img\n",
    "\n",
    "    def make_image_rgb(self, image):\n",
    "        if image.image.mode == \"RGB\":\n",
    "            return image\n",
    "        return image.image.convert(\"RGB\")\n",
    "\n",
    "    def pad_image_to_square(self, image):\n",
    "        if isinstance(image, ImageTensor):\n",
    "            image.image = self.pad_image_to_square(image.image)\n",
    "            return image\n",
    "        else:\n",
    "            h, w = image.shape[2:]\n",
    "            max_dim = max(h, w)\n",
    "            pad_h = max_dim - h\n",
    "            pad_w = max_dim - w\n",
    "            image = np.pad(image, ((0,0), (0,0), (0,pad_h), (0,pad_w)), mode=\"constant\", constant_values=0)\n",
    "            return image\n",
    "\n",
    "    def normalize_image(self, image):\n",
    "        if isinstance(image, ImageTensor):\n",
    "            image.image = self.normalize_image(image.image)\n",
    "            return image\n",
    "        else:\n",
    "            image = (image - self.mean) / self.std\n",
    "            return image\n",
    "\n",
    "    def to_tensor(self, image):\n",
    "        if isinstance(image, ImageTensor):\n",
    "            image.image = self.to_tensor(image.image)\n",
    "            return image\n",
    "        else:\n",
    "            image = image.transpose(2,0,1)[None,:,:,:].astype(np.float32)\n",
    "            return image\n",
    "\n",
    "    def from_image_to_input(self, image):\n",
    "        image = self.make_image_rgb(image)\n",
    "        image = self.resize_image_to_long_side(image)\n",
    "        image = self.normalize_image(image)\n",
    "        image = self.to_tensor(image)\n",
    "        # pad to square\n",
    "        if self.pad_to_square:\n",
    "            image = self.pad_image_to_square(image)\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the image of the path into an ndarray using PIL and set it as the image to be processed\n",
    "image_path = R\"C:\\Users\\mmoller\\OneDrive - NVIDIA Corporation\\Pictures\\Camera Roll\\WIN_20240318_17_50_49_Pro.jpg\"\n",
    "image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image collections for plotting and testing\n",
    "image_original = ImageTensor(image)\n",
    "image_object = ImageTensor(image)\n",
    "\n",
    "# image processor object\n",
    "image_preprocessor = ImagePreprocessor()\n",
    "\n",
    "image_rgb = image_preprocessor.make_image_rgb(image_object)\n",
    "numpy_image = np.array(image_rgb.image)\n",
    "input_image = image_preprocessor.from_image_to_input(image_object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "# https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\n",
    "# https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
    "\n",
    "class ModelDownloader:\n",
    "    def __init__(self, download_url:str=None) -> None:\n",
    "        if download_url is None:\n",
    "            download_url = R\"https://dl.fbaipublicfiles.com/segment_anything/\"\n",
    "        self.models = {\n",
    "            \"sam_vit_h\": download_url + \"sam_vit_h_4b8939.pth\",\n",
    "            \"sam_vit_b\": download_url + \"sam_vit_b_01ec64.pth\",\n",
    "            \"sam_vit_l\": download_url + \"sam_vit_l_0b3195.pth\",\n",
    "        }\n",
    "\n",
    "    def download_model(self, model_type, filepath:str=None):\n",
    "        if filepath is None:\n",
    "            # create a download folder in the current directory and use it to save the models there\n",
    "            filedir = os.path.join(os.getcwd(), \"downloads\")\n",
    "            os.makedirs(filedir, exist_ok=True)\n",
    "            filepath = os.path.join(filedir, f\"{model_type}.pth\")\n",
    "        if model_type not in self.models:\n",
    "            raise ValueError(\"Invalid model_type\")\n",
    "\n",
    "        url = self.models[model_type]\n",
    "        wget.download(url, filepath)\n",
    "\n",
    "        return filepath\n",
    "\n",
    "downloader = ModelDownloader()\n",
    "for model_type in [\"sam_vit_h\", \"sam_vit_b\", \"sam_vit_l\"]:\n",
    "    downloaded_filepath = downloader.download_model(model_type)\n",
    "    print(f\"Model downloaded to: {downloaded_filepath}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch model usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a model\n",
    "# !python -m wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
    "sam = sam_model_registry[\"vit_b\"](R\"C:\\Users\\mmoller\\Downloads\\sam_vit_b_01ec64.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting a query point on the image\n",
    "input_point = np.array([[1000, 600]])\n",
    "input_label = np.array([1])\n",
    "# format of the box is [x0, y0, x1, y1], where o is the top left corner and 1 is the bottom right corner\n",
    "input_box = np.array([800, 150, 1250, 800])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image_rgb.image)\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "show_box(input_box, plt.gca())\n",
    "plt.axis('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_rgb.image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SamPredictor(sam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.set_image(numpy_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "masks = mask_generator.generate(numpy_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(image)\n",
    "show_anns(masks)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export encoder and decoder to onnx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"vit_b\"\n",
    "checkpoint_path = os.path.join(os.getcwd(), \"downloads\", f\"sam_{model_type}.pth\")\n",
    "onnx_decoder_path = os.path.join(os.getcwd(), \"onnx\", f\"sam_{model_type}_query_decoder.onnx\")\n",
    "onnx_encoder_path = os.path.join(os.getcwd(), \"onnx\", f\"sam_{model_type}_encoder.onnx\")\n",
    "if not os.path.exists(os.path.dirname(onnx_decoder_path)):\n",
    "    os.makedirs(os.path.dirname(onnx_decoder_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = sam_model_registry[model_type](checkpoint=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = SamOnnxModel(sam, return_single_mask=True)\n",
    "\n",
    "dynamic_axes = {\n",
    "    \"point_coords\": {1: \"num_points\"},\n",
    "    \"point_labels\": {1: \"num_points\"},\n",
    "}\n",
    "\n",
    "embed_dim = sam.prompt_encoder.embed_dim\n",
    "embed_size = sam.prompt_encoder.image_embedding_size\n",
    "mask_input_size = [4 * x for x in embed_size]\n",
    "dummy_inputs = {\n",
    "    \"image_embeddings\": torch.randn(1, embed_dim, *embed_size, dtype=torch.float),\n",
    "    \"point_coords\": torch.randint(low=0, high=1024, size=(1, 5, 2), dtype=torch.float),\n",
    "    \"point_labels\": torch.randint(low=0, high=4, size=(1, 5), dtype=torch.float),\n",
    "    \"mask_input\": torch.randn(1, 1, *mask_input_size, dtype=torch.float),\n",
    "    \"has_mask_input\": torch.tensor([1], dtype=torch.float),\n",
    "    \"orig_im_size\": torch.tensor([1500, 2250], dtype=torch.float),\n",
    "}\n",
    "output_names = [\"masks\", \"iou_predictions\", \"low_res_masks\"]\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    with open(onnx_decoder_path, \"wb\") as f:\n",
    "        torch.onnx.export(\n",
    "            onnx_model,\n",
    "            tuple(dummy_inputs.values()),\n",
    "            f,\n",
    "            export_params=True,\n",
    "            verbose=False,\n",
    "            opset_version=17,\n",
    "            do_constant_folding=True,\n",
    "            input_names=list(dummy_inputs.keys()),\n",
    "            output_names=output_names,\n",
    "            dynamic_axes=dynamic_axes,\n",
    "        )\n",
    "\n",
    "    with open(onnx_encoder_path, \"wb\") as f:\n",
    "    # Export images encoder from SAM model to ONNX\n",
    "        torch.onnx.export(\n",
    "            f=f,\n",
    "            model=sam.image_encoder,\n",
    "            args=torch.randn(1, 3, 1024, 1024),\n",
    "            input_names=[\"images\"],\n",
    "            output_names=[\"embeddings\"],\n",
    "            export_params=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using the local onnx model decoder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
    "ort_session = onnxruntime.InferenceSession(onnx_decoder_path)\n",
    "sam.to(device='cuda')\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.set_image(numpy_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "masks = mask_generator.generate(numpy_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "show_anns(masks)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing encoder and decoder on an image\n",
    "encoder = onnxruntime.InferenceSession(onnx_encoder_path, providers=[\"CUDAExecutionProvider\"])\n",
    "decoder = onnxruntime.InferenceSession(onnx_decoder_path, providers=[\"CUDAExecutionProvider\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = encoder.run(None, {\"images\": input_image.image})\n",
    "embeddings = outputs[0]\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in outputs:\n",
    "    print(type(output), output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instructions from https://github.com/facebookresearch/segment-anything/blob/main/notebooks/onnx_model_example.ipynb\n",
    "The ONNX model has a different input signature than SamPredictor.predict. The following inputs must all be supplied. Note the special cases for both point and mask inputs. All inputs are np.float32.\n",
    "\n",
    "- image_embeddings: The image embedding from predictor.get_image_embedding(). Has a batch index of length 1.\n",
    "- point_coords: Coordinates of sparse input prompts, corresponding to both point inputs and box inputs. Boxes are encoded using two points, one for the top-left corner and one for the bottom-right corner. Coordinates must already be transformed to long-side 1024. Has a batch index of length 1.\n",
    "- point_labels: Labels for the sparse input prompts. 0 is a negative input point, 1 is a positive input point, 2 is a top-left box corner, 3 is a bottom-right box corner, and -1 is a padding point. If there is no box input, a single padding point with label -1 and coordinates (0.0, 0.0) should be concatenated.\n",
    "- mask_input: A mask input to the model with shape 1x1x256x256. This must be supplied even if there is no mask input. In this case, it can just be zeros.\n",
    "- has_mask_input: An indicator for the mask input. 1 indicates a mask input, 0 indicates no mask input.\n",
    "- orig_im_size: The size of the input image in (H,W) format, before any transformation.\n",
    "\n",
    "Additionally, the ONNX model does not threshold the output mask logits. To obtain a binary mask, threshold at sam.mask_threshold (equal to 0.0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_size = np.array([image_rgb.orig_width, image_rgb.orig_height ], dtype=np.float32)\n",
    "\n",
    "\n",
    "#plotting a query point on the image\n",
    "input_point = np.array([[1000, 600]])\n",
    "# label 1 means foreground, 0 means background\n",
    "input_label = np.array([1])\n",
    "# format of the box is [x0, y0, x1, y1], where o is the top left corner and 1 is the bottom right corner\n",
    "input_box = np.array([800, 150, 1250, 800])\n",
    "box_labels = np.array([2, 3])\n",
    "\n",
    "onnx_box_coords = input_box.reshape(2, 2)\n",
    "onnx_box_labels = np.array([2,3])\n",
    "\n",
    "onnx_coord = np.concatenate([input_point, onnx_box_coords], axis=0)[None, :, :]\n",
    "onnx_label = np.concatenate([input_label, onnx_box_labels], axis=0)[None, :].astype(np.float32)\n",
    "onnx_coord = input_image.apply_coords(onnx_coord).astype(np.float32)\n",
    "# onnx_coord = predictor.transform.apply_coords(onnx_coord, image_size).astype(np.float32)\n",
    "\n",
    "onnx_mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n",
    "onnx_has_mask_input = np.zeros(1, dtype=np.float32)\n",
    "\n",
    "orig_im_size = np.array([input_image.orig_height, input_image.orig_width], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = decoder.run(None,{\n",
    "    \"image_embeddings\": embeddings,\n",
    "    \"point_coords\": onnx_coord,\n",
    "    \"point_labels\": onnx_label,\n",
    "    \"mask_input\": onnx_mask_input,\n",
    "    \"has_mask_input\": onnx_has_mask_input,\n",
    "    \"orig_im_size\": orig_im_size,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = outputs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(masks[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(masks[0,0].flatten(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = masks[0,0,...]\n",
    "mask = mask > 0\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image_original.image)\n",
    "show_mask(mask, plt.gca())\n",
    "show_box(input_box, plt.gca())\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
