{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from polygraphy.backend.trt import util as trt_util\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "from cuda import cudart\n",
    "from copy import deepcopy\n",
    "import tensorrt as trt\n",
    "\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n",
    "\n",
    "\n",
    "def show_mask(mask, ax):\n",
    "    color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n",
    "\n",
    "\n",
    "class ImageTensor:\n",
    "    def __init__(self, image):\n",
    "        self.image = image\n",
    "        self.orig_width, self.orig_height = image.size\n",
    "        self.resized_width, self.resized_height = None, None\n",
    "        self.pad_width, self.pad_height = None, None\n",
    "\n",
    "    def size(self):\n",
    "        return self.image.size\n",
    "\n",
    "    def apply_coords(self, coords: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Expects a numpy array of length 2 in the final dimension\n",
    "        \"\"\"\n",
    "        old_h, old_w = self.orig_height, self.orig_width\n",
    "        new_h, new_w = self.resized_height, self.resized_width\n",
    "        coords = deepcopy(coords).astype(float)\n",
    "        coords[..., 0] = coords[..., 0] * (new_w / old_w)\n",
    "        coords[..., 1] = coords[..., 1] * (new_h / old_h)\n",
    "        return coords\n",
    "\n",
    "\n",
    "class ImagePreprocessor:\n",
    "    def __init__(self, long_side_max=1024, mean=None, std=None, image_format=\"RGB\", pad_to_square=True):\n",
    "        self.long_side_max = long_side_max\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.image_format = image_format\n",
    "        self.pad_to_square = pad_to_square\n",
    "        if self.mean is None:\n",
    "            self.mean = np.array([123.675, 116.28, 103.53])\n",
    "        if self.std is None:\n",
    "            self.std = np.array([58.395, 57.12, 57.375])\n",
    "\n",
    "\n",
    "    def resize_image_to_long_side(self, img: ImageTensor):\n",
    "        if self.long_side_max is None:\n",
    "            return img\n",
    "        orig_width, orig_height = img.image.size\n",
    "        if orig_width > orig_height:\n",
    "            img.resized_width = self.long_side_max\n",
    "            img.resized_height = int(self.long_side_max / orig_width * orig_height)\n",
    "        else:\n",
    "            img.resized_height = self.long_side_max\n",
    "            img.resized_width = int(self.long_side_max / orig_height * orig_width)\n",
    "\n",
    "        img.image = img.image.resize((img.resized_width, img.resized_height), Image.Resampling.BILINEAR)\n",
    "        return img\n",
    "\n",
    "    def make_image_rgb(self, image):\n",
    "        if image.image.mode == \"RGB\":\n",
    "            return image\n",
    "        else:\n",
    "            image.image = image.image.convert(\"RGB\")\n",
    "            return image\n",
    "\n",
    "    def pad_image_to_square(self, image):\n",
    "        if isinstance(image, ImageTensor):\n",
    "            image.image = self.pad_image_to_square(image.image)\n",
    "            return image\n",
    "        else:\n",
    "            h, w = image.shape[2:]\n",
    "            max_dim = max(h, w)\n",
    "            pad_h = max_dim - h\n",
    "            pad_w = max_dim - w\n",
    "            image = np.pad(image, ((0,0), (0,0), (0,pad_h), (0,pad_w)), mode=\"constant\", constant_values=0)\n",
    "            return image\n",
    "\n",
    "    def normalize_image(self, image):\n",
    "        if isinstance(image, ImageTensor):\n",
    "            image.image = self.normalize_image(image.image)\n",
    "            return image\n",
    "        else:\n",
    "            image = (image - self.mean) / self.std\n",
    "            return image\n",
    "\n",
    "    def to_tensor(self, image):\n",
    "        if isinstance(image, ImageTensor):\n",
    "            image.image = self.to_tensor(image.image)\n",
    "            return image\n",
    "        else:\n",
    "            image = image.transpose(2,0,1)[None,:,:,:].astype(np.float32)\n",
    "            return image\n",
    "\n",
    "    def from_image_to_input(self, image):\n",
    "        image = self.make_image_rgb(image)\n",
    "        image = self.resize_image_to_long_side(image)\n",
    "        image = self.normalize_image(image)\n",
    "        image = self.to_tensor(image)\n",
    "        # pad to square\n",
    "        if self.pad_to_square:\n",
    "            image = self.pad_image_to_square(image)\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"engine_fp32/decoder.engine\", \"rb\")\n",
    "runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))\n",
    "\n",
    "decoder_engine = runtime.deserialize_cuda_engine(f.read())\n",
    "decoder_context = decoder_engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"engine_fp32/encoder.engine\", \"rb\")\n",
    "encoder_engine = runtime.deserialize_cuda_engine(f.read())\n",
    "encoder_context = encoder_engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open sample file\n",
    "image_path = R\"downloads/sample.png\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# image collections for plotting and testing\n",
    "image_original = ImageTensor(image)\n",
    "image_object = ImageTensor(image)\n",
    "\n",
    "# image processor object\n",
    "image_preprocessor = ImagePreprocessor()\n",
    "\n",
    "image_rgb = image_preprocessor.make_image_rgb(image_object)\n",
    "numpy_image = np.array(image_rgb.image)\n",
    "input_image = image_preprocessor.from_image_to_input(image_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image.resized_height, input_image.resized_width, input_image.orig_height, input_image.orig_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(input_image.image[0,0,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = input_image.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor.shape, input_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "BATCH_SIZE = 1\n",
    "USE_FP16 = False\n",
    "target_dtype = np.float16 if USE_FP16 else np.float32\n",
    "# need to set input and output precisions to FP16 to fully enable it\n",
    "output = np.empty([BATCH_SIZE, 256, 64, 64], dtype = target_dtype) \n",
    "\n",
    "# allocate device memory for encoder\n",
    "d_input = cuda.mem_alloc(1 * input_tensor.nbytes)\n",
    "d_output = cuda.mem_alloc(1 * output.nbytes)\n",
    "bindings = [int(d_input), int(d_output)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = cuda.Stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_encoder(batch): # result gets copied into output\n",
    "    # transfer input data to device\n",
    "    cuda.memcpy_htod_async(d_input, batch, stream)\n",
    "    # execute model\n",
    "    encoder_context.execute_async_v2(bindings, stream.handle, None)\n",
    "    # transfer predictions back\n",
    "    cuda.memcpy_dtoh_async(output, d_output, stream)\n",
    "    # syncronize threads\n",
    "    stream.synchronize()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Warming up...\")\n",
    "pred = predict_encoder(input_tensor)\n",
    "print(\"Done warming up!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "pred = predict_encoder(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.dtype, type(pred), pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the input embedding\n",
    "plt.imshow(pred[0,0,:42,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_size = np.array([image_rgb.orig_width, image_rgb.orig_height ], dtype=np.float32)\n",
    "# #plotting a query point on the image\n",
    "# input_point = np.array([[300, 300]])\n",
    "# # label 1 means foreground, 0 means background\n",
    "# input_label = np.array([1])\n",
    "# # format of the box is [x0, y0, x1, y1], where o is the top left corner and 1 is the bottom right corner\n",
    "# input_box = np.array([400, 250, 2500, 1500])\n",
    "# box_labels = np.array([2, 3])\n",
    "\n",
    "# onnx_box_coords = input_box.reshape(2, 2)\n",
    "# onnx_box_labels = np.array([2,3])\n",
    "\n",
    "# onnx_coord = np.concatenate([input_point, onnx_box_coords], axis=0)[None, :, :]\n",
    "# onnx_label = np.concatenate([input_label, onnx_box_labels], axis=0)[None, :].astype(np.float32)\n",
    "# onnx_coord = input_image.apply_coords(onnx_coord).astype(np.float32)\n",
    "# # onnx_coord = predictor.transform.apply_coords(onnx_coord, image_size).astype(np.float32)\n",
    "\n",
    "# onnx_mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n",
    "# onnx_has_mask_input = np.zeros(1, dtype=np.float32)\n",
    "\n",
    "# decoder_params = {\n",
    "#     \"image_embeddings\": pred,\n",
    "#     \"point_coords\": onnx_coord,\n",
    "#     \"point_labels\": onnx_label,\n",
    "#     \"mask_input\": onnx_mask_input,\n",
    "#     \"has_mask_input\": onnx_has_mask_input,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_dict(batch, points):\n",
    "    embed_dim = 256\n",
    "    embed_size = (64, 64)\n",
    "    mask_input_size = [4 * x for x in embed_size]\n",
    "\n",
    "    return {\n",
    "        \"image_embeddings\": (1, embed_dim, *embed_size),\n",
    "        \"point_coords\": (batch, points, 2),\n",
    "        \"point_labels\": (batch, points),\n",
    "        \"mask_input\": (1, 1, *mask_input_size),\n",
    "        \"has_mask_input\": (1, ),\n",
    "        \"iou_predictions\": (batch, 4), # up to 4 masks per point\n",
    "        \"low_res_masks\": (batch, 4, *mask_input_size), # up to 4 masks per point\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_param_dict(embeddings, image_embedding_size, device):\n",
    "    input_point = torch.as_tensor(np.array([[300,300]]), device=device, dtype=torch.float32)\n",
    "    input_label = torch.as_tensor(np.array([1]), device=device, dtype=torch.float32)\n",
    "    zero_point = torch.zeros(1, 2, device=device, dtype=torch.float32)\n",
    "    negative_label = torch.as_tensor(np.array([-1]), device=device, dtype=torch.float32)\n",
    "    coord = torch.cat((input_point, zero_point), dim=0)[None, :, :]\n",
    "    label = torch.cat((input_label, negative_label), dim=0)[None, :]\n",
    "    dtype = torch.float32\n",
    "\n",
    "    embeddings = torch.as_tensor(embeddings, dtype=dtype)\n",
    "    coord = input_point.to(dtype=dtype)\n",
    "    label = input_label.to(dtype=dtype)\n",
    "\n",
    "    mask_input = torch.zeros(1, 1, 4 * image_embedding_size[0], \n",
    "                                4 * image_embedding_size[1], device=device, dtype=dtype)\n",
    "    has_mask_input = torch.zeros(1, dtype=dtype, device=device)\n",
    "\n",
    "\n",
    "    params = {\n",
    "        \"image_embeddings\": embeddings,\n",
    "        \"point_coords\": coord,\n",
    "        \"point_labels\": label,\n",
    "        \"mask_input\": mask_input,\n",
    "        \"has_mask_input\": has_mask_input,\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = OrderedDict()\n",
    "def _allocate_buffers(shape_dict, engine, context):\n",
    "    for idx in range(trt_util.get_bindings_per_profile(engine)):\n",
    "        \n",
    "        binding = engine[idx]\n",
    "        if shape_dict and binding in shape_dict:\n",
    "            shape = shape_dict[binding]\n",
    "        else:\n",
    "            shape = engine.get_binding_shape(binding)\n",
    "        dtype = trt.nptype(engine.get_tensor_dtype(binding))\n",
    "        if engine.get_tensor_mode(binding) == trt.TensorIOMode.INPUT:\n",
    "            context.set_input_shape( binding, shape)\n",
    "        tensor = torch.empty(tuple(shape), dtype=torch.float32).to(device=device)\n",
    "        tensors[binding] = tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dict = get_shape_dict(batch=1, points=2)\n",
    "print(decoder_dict)\n",
    "decoder_params = get_param_dict(embeddings=pred, image_embedding_size=(64, 64), device=device)\n",
    "for key, value in decoder_params.items():\n",
    "    print(key, value.shape)\n",
    "for key, value in tensors.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_allocate_buffers(decoder_dict, decoder_engine, decoder_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CUASSERT(cuda_ret):\n",
    "    err = cuda_ret[0]\n",
    "    if err != cudart.cudaError_t.cudaSuccess:\n",
    "         raise RuntimeError(f\"CUDA ERROR: {err}, error code reference: https://nvidia.github.io/cuda-python/module/cudart.html#cuda.cudart.cudaError_t\")\n",
    "    if len(cuda_ret) > 1:\n",
    "        return cuda_ret[1]\n",
    "    return None\n",
    "\n",
    "def infer_engine(stream, tensors, param_dict, context, use_cuda_graph=True, cuda_graph_instance=None):\n",
    "    for name, buf in param_dict.items():\n",
    "        tensors[name].copy_(buf)\n",
    "\n",
    "    for name, tensor in tensors.items():\n",
    "        context.set_tensor_address(name, tensor.data_ptr())\n",
    "\n",
    "    if use_cuda_graph:\n",
    "        if cuda_graph_instance is not None:\n",
    "            CUASSERT(cudart.cudaGraphLaunch(cuda_graph_instance, stream.handle))\n",
    "            CUASSERT(cudart.cudaStreamSynchronize(stream.handle))\n",
    "        else:\n",
    "            # do inference before CUDA graph capture\n",
    "            noerror = context.execute_async_v3(stream.handle)\n",
    "            if not noerror:\n",
    "                raise ValueError(f\"ERROR: inference failed.\")\n",
    "            # capture cuda graph\n",
    "            CUASSERT(cudart.cudaStreamBeginCapture(stream.handle, cudart.cudaStreamCaptureMode.cudaStreamCaptureModeGlobal))\n",
    "            context.execute_async_v3(stream.handle)\n",
    "            graph = CUASSERT(cudart.cudaStreamEndCapture(stream.handle))\n",
    "            cuda_graph_instance = CUASSERT(cudart.cudaGraphInstantiate(graph, 0))\n",
    "    else:\n",
    "        noerror = context.execute_async_v3(stream.handle)\n",
    "        if not noerror:\n",
    "            raise ValueError(f\"ERROR: inference failed.\")\n",
    "        \n",
    "    stream.synchronize()\n",
    "\n",
    "\n",
    "    return tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = infer_engine(stream, tensors, decoder_params, decoder_context, use_cuda_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in outputs.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = outputs['low_res_masks']\n",
    "scores = outputs['iou_predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(masks.dtype, masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def upsample_image(image, new_size):\n",
    "    if not isinstance(image, torch.Tensor):\n",
    "        image_tensor = to_tensor(image)\n",
    "    else:\n",
    "        image_tensor = image\n",
    "    \n",
    "    image_tensor = image_tensor.unsqueeze(0).to(torch.device('cuda'))\n",
    "    upsampled_tensor = torch.nn.functional.interpolate(image_tensor, size=new_size, mode='bilinear', align_corners=False)\n",
    "    upsampled_tensor = upsampled_tensor.squeeze(0).to(torch.device('cpu')).numpy()\n",
    "    \n",
    "    return upsampled_tensor\n",
    "\n",
    "def crop_image_to_prepadded_size(image, padding_size):\n",
    "    width, height = image.size\n",
    "    left = padding_size\n",
    "    top = padding_size\n",
    "    right = width - padding_size\n",
    "    bottom = height - padding_size\n",
    "    cropped_image = image.crop((left, top, right, bottom))\n",
    "    return cropped_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_masks = masks[:,3, :, :]\n",
    "upsampled_mask = upsample_image(sliced_masks, (1024, 1024))\n",
    "print(upsampled_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop to prepadded size\n",
    "padding_size = np.asarray(upsampled_mask.shape[-2:])- np.asarray(image_original.image.size)\n",
    "cropped_mask = upsampled_mask[0, :-padding_size[0], :-padding_size[1]]\n",
    "cropped_mask = cropped_mask > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cropped_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(cropped_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mask_postprocessing(self, masks: torch.Tensor, orig_im_size) -> torch.Tensor:\n",
    "    orig_im_size_torch = torch.tensor(orig_im_size, device=self.device)\n",
    "    masks = F.interpolate(\n",
    "        masks,\n",
    "        size=(1024, 1024),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    \n",
    "    masks = F.interpolate(masks, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upscaled_masks = self._mask_postprocessing(masks, orig_im_size)\n",
    "\n",
    "upscaled_masks = upscaled_masks > self.sam.mask_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
